--- kernel/sched/bfs.c
+++ kernel/sched/bfs.c
@@ -812,60 +824,6 @@
 	);
 }
 
-/*
- * closest_mask_cpu() doesn't check for the original cpu,
- * caller should ensure cpumask is not empty
- */
-static int closest_mask_cpu(int cpu, cpumask_t *cpumask)
-{
-	cpumask_t tmpmask, non_scaled_mask;
-	cpumask_t *res_mask = &tmpmask;
-
-	if (cpumask_and(&non_scaled_mask, cpumask, &grq.non_scaled_cpumask)) {
-		/*
-		 * non_scaled llc cpus checking
-		 */
-		if (llc_cpu_check(cpu, &non_scaled_mask, res_mask))
-			return cpumask_first(res_mask);
-		/*
-		 * scaling llc cpus checking
-		 */
-		if (llc_cpu_check(cpu, cpumask, res_mask))
-			return cpumask_first(res_mask);
-
-		/*
-		 * non_scaled non_llc cpus checking
-		 */
-		if (nonllc_cpu_check(cpu, &non_scaled_mask, res_mask))
-			return cpumask_first(res_mask);
-		/*
-		 * scaling non_llc cpus checking
-		 */
-		if (nonllc_cpu_check(cpu, cpumask, res_mask))
-			return cpumask_first(res_mask);
-
-		/* All cpus avariable */
-
-		return cpumask_first(cpumask);
-	}
-
-	/*
-	 * scaling llc cpus checking
-	 */
-	if (llc_cpu_check(cpu, cpumask, res_mask))
-		return cpumask_first(res_mask);
-
-	/*
-	 * scaling non_llc cpus checking
-	 */
-	if (nonllc_cpu_check(cpu, cpumask, res_mask))
-		return cpumask_first(res_mask);
-
-	/* All cpus avariable */
-
-	return cpumask_first(cpumask);
-}
-
 static inline int best_mask_cpu(const int cpu, cpumask_t *cpumask)
 {
 	cpumask_t tmpmask, non_scaled_mask;
@@ -1002,44 +960,22 @@
 #endif
 #endif
 
-static inline bool resched_best_idle(struct task_struct *p)
+static inline struct rq *task_best_idle_rq(struct task_struct *p)
 {
         cpumask_t check_cpumask;
 
         if (cpumask_and(&check_cpumask, &p->cpus_allowed, &grq.cpu_idle_map)) {
-                int best_cpu;
+		int best_cpu;
 
                 best_cpu = best_mask_cpu(task_cpu(p), &check_cpumask);
 #ifdef CONFIG_SMT_NICE
 		if (!smt_should_schedule(p, best_cpu))
-			return false;
+			return NULL;
 #endif
-		resched_curr(cpu_rq(best_cpu));
-		return true;
+		return cpu_rq(best_cpu);
 	}
-	return false;
-}
-
-/* Reschedule the best idle CPU that is not this one. */
-static bool
-resched_closest_idle(struct rq *rq, int cpu, struct task_struct *p)
-{
-        cpumask_t check_cpumask;
-
-	cpumask_copy(&check_cpumask, &p->cpus_allowed);
-	cpumask_clear_cpu(cpu, &check_cpumask);
-        if (cpumask_and(&check_cpumask, &check_cpumask, &grq.cpu_idle_map)) {
-                int best_cpu;
 
-                best_cpu = closest_mask_cpu(task_cpu(p), &check_cpumask);
-#ifdef CONFIG_SMT_NICE
-		if (!smt_should_schedule(p, best_cpu))
-			return false;
-#endif
-		resched_curr(cpu_rq(best_cpu));
-		return true;
-	}
-	return false;
+	return NULL;
 }
 
 /*
@@ -1090,9 +1026,9 @@
 {
 }
 
-static inline bool resched_best_idle(struct task_struct *p)
+static inline struct rq *task_best_idle_rq(struct task_struct *p)
 {
-	return false;
+	return NULL;
 }
 
 static inline bool suitable_idle_cpus(struct task_struct *p)
@@ -1529,17 +1465,10 @@
 	return !cpumask_test_cpu(cpu, &p->cpus_allowed);
 }
 
-/*
- * When all else is equal, still prefer this_rq.
- *
- * @this_rq is not used
- * In all cases, try_preempt() is called with grq lock held. To simplify
- * per rq->rq_prio/rq->rq_deadline access, grq lock is used.
- * That means set_rq_task()/reset_rq_task() also needs grq lock.
- */
-static void try_preempt(struct task_struct *p, struct rq *this_rq)
+static struct rq* task_preemptable_rq(struct task_struct *p)
 {
 	int cpu, target_cpu;
+	struct rq *target_rq;
 	u64 highest_priodl;
 	cpumask_t tmp;
 
@@ -1550,15 +1479,16 @@
 	 */
 	clear_sticky(p);
 
-	if (resched_best_idle(p))
-		return;
+	target_rq = task_best_idle_rq(p);
+	if (target_rq)
+		return target_rq;
 
 	/* IDLEPRIO tasks never preempt anything but idle */
 	if (p->policy == SCHED_IDLEPRIO)
-		return;
+		return NULL;
 
 	if (unlikely(!cpumask_and(&tmp, cpu_online_mask, &p->cpus_allowed)))
-		return;
+		return NULL;
 
 	target_cpu = cpu = cpumask_first(&tmp);
 #if defined(CONFIG_SMP) && !defined(CONFIG_64BIT)
@@ -1590,7 +1520,9 @@
 		return NULL;
 #endif
 	if (can_preempt(p, highest_priodl))
-		resched_curr(cpu_rq(target_cpu));
+		return cpu_rq(target_cpu);
+
+	return NULL;
 }
 #else /* CONFIG_SMP */
 static inline bool needs_other_cpu(struct task_struct *p, int cpu)
@@ -1680,15 +1611,6 @@
 	 */
 	if (p->flags & PF_WQ_WORKER)
 		wq_worker_waking_up(p, cpu_of(rq));
-
-	/*
-	 * Sync wakeups (i.e. those types of wakeups where the waker
-	 * has indicated that it will leave the CPU in short order)
-	 * don't trigger a preemption if there are no idle cpus,
-	 * instead waiting for current to deschedule.
-	 */
-	if (!is_sync || suitable_idle_cpus(p))
-		try_preempt(p, rq);
 }
 
 /*
@@ -2149,6 +2112,11 @@
 		kprobe_flush_task(prev);
 		put_task_struct(prev);
 	}
+
+	preempt_rq(prq);
+	preempt_rq(w_prq);
+	preempt_rq(us_prq);
+
 	return rq;
 }
 
@@ -4013,13 +3977,15 @@
 	update_task_priodl(p);
 	if (queued) {
 		enqueue_task(p, rq);
-		try_preempt(p, rq);
+		prq = task_preemptable_rq(p);
 	}
 
 	check_task_changed(rq, p, oldprio);
 
 out_unlock:
 	task_vrq_unlock_irqrestore(rq, lock, &flags);
+
+	preempt_rq(prq);
 }
 
 #endif
@@ -4282,7 +4250,7 @@
 	int queued, retval, oldprio, oldpolicy = -1;
 	int policy = attr->sched_policy;
 	unsigned long flags;
-	struct rq *rq;
+	struct rq *rq, *prq = NULL;
 	int reset_on_fork;
 	raw_spinlock_t *lock;
 
@@ -4438,7 +4406,7 @@
 	__setscheduler(rq, p, attr);
 	if (queued) {
 		enqueue_task(p, rq);
-		try_preempt(p, rq);
+		prq = task_preemptable_rq(p);
 	}
 
 	check_task_changed(rq, p, oldprio);
@@ -7528,31 +7508,35 @@
 	__setscheduler(rq, p, &attr);
 	if (queued) {
 		enqueue_task(p, rq);
-		try_preempt(p, rq);
+		prq = task_preemptable_rq(p);
 	}
 
 	check_task_changed(rq, p, old_prio);
+
+	task_vrq_unlock(rq, lock);
+	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+
+	preempt_rq(prq);
 }
 
 void normalize_rt_tasks(void)
 {
 	struct task_struct *g, *p;
-	unsigned long flags;
 	struct rq *rq;
-	raw_spinlock_t *lock;
 
 	read_lock(&tasklist_lock);
 	for_each_process_thread(g, p) {
-		if (!rt_task(p) && !iso_task(p))
+		if (!rt_task(p) && !iso_task(p)) {
+			/*
+			 * Renice negative nice level userspace
+			 * tasks back to 0:
+			 */
+			if (task_nice(p) < 0 && p->mm)
+				set_user_nice(p, 0);
 			continue;
-
-		raw_spin_lock_irqsave(&p->pi_lock, flags);
-		rq = task_vrq_lock(p, &lock);
+		}
 
 		normalize_task(rq, p);
-
-		task_vrq_unlock(rq, lock);
-		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 	}
 	read_unlock(&tasklist_lock);
 }
